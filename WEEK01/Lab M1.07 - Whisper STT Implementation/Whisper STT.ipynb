{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0787f02a",
      "metadata": {},
      "source": [
        "LAB M.107  Whisper STT Implementation\n",
        "CINDY LUND\n",
        "\n",
        "This notebook implements a complete speech-to-text pipeline using OpenAI Whisper, including:\n",
        "- basic, prompted, and unprompted transcription\n",
        "- audio chunking for long recordings\n",
        "- timestamp extraction with chunk offsets\n",
        "- exports to TXT, JSON, and SRT formats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8aa21cdd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in c:\\users\\cindy\\anaconda3\\envs\\ac\\lib\\site-packages (2.17.0)\n",
            "Requirement already satisfied: pydub in c:\\users\\cindy\\anaconda3\\envs\\ac\\lib\\site-packages (0.25.1)\n",
            "Requirement already satisfied: audioop-lts in c:\\users\\cindy\\anaconda3\\envs\\ac\\lib\\site-packages (0.2.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\cindy\\anaconda3\\envs\\ac\\lib\\site-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\cindy\\anaconda3\\envs\\ac\\lib\\site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\cindy\\anaconda3\\envs\\ac\\lib\\site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\cindy\\anaconda3\\envs\\ac\\lib\\site-packages (from openai) (0.13.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\cindy\\anaconda3\\envs\\ac\\lib\\site-packages (from openai) (2.12.5)\n",
            "Requirement already satisfied: sniffio in c:\\users\\cindy\\anaconda3\\envs\\ac\\lib\\site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\cindy\\anaconda3\\envs\\ac\\lib\\site-packages (from openai) (4.67.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\cindy\\anaconda3\\envs\\ac\\lib\\site-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\cindy\\anaconda3\\envs\\ac\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in c:\\users\\cindy\\anaconda3\\envs\\ac\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\cindy\\anaconda3\\envs\\ac\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\cindy\\anaconda3\\envs\\ac\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\cindy\\anaconda3\\envs\\ac\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\cindy\\anaconda3\\envs\\ac\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\cindy\\anaconda3\\envs\\ac\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\cindy\\anaconda3\\envs\\ac\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install openai pydub audioop-lts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "6afd77dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Setup directories for audio and transcripts\n",
        "   \n",
        "import os\n",
        "os.makedirs(\"audio\", exist_ok=True)\n",
        "os.makedirs(\"transcripts\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c1da0d2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from pydub import AudioSegment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "11505462",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "609b4297",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API key loaded: True\n"
          ]
        }
      ],
      "source": [
        "print(\"API key loaded:\", bool(os.getenv(\"OPENAI_API_KEY\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "fd02051b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ME025.mp3', 'not used - no difference between guided and unguided']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.listdir(\"audio\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "35df58d7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Using meeting sample: ME025.mp3\n",
            "‚è± Duration: 100.0 seconds\n",
            "üì¶ Channels: 2\n",
            "üéö Frame rate: 44100 Hz\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from pydub import AudioSegment\n",
        "\n",
        "audio_dir = Path(\"audio\")\n",
        "audio_path = audio_dir / \"ME025.mp3\"\n",
        "\n",
        "if not audio_path.exists():\n",
        "    raise FileNotFoundError(f\"‚ùå {audio_path.name} not found in ./audio\")\n",
        "\n",
        "audio = AudioSegment.from_file(audio_path)\n",
        "\n",
        "print(f\"‚úÖ Using meeting sample: {audio_path.name}\")\n",
        "print(f\"‚è± Duration: {audio.duration_seconds:.1f} seconds\")\n",
        "print(f\"üì¶ Channels: {audio.channels}\")\n",
        "print(f\"üéö Frame rate: {audio.frame_rate} Hz\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f349508c",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Basic Transcription (without chunking)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd3a9b37",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Transcribing the first 30 seconds (no chunking, no prompt)...\n",
            "\n",
            "üìù Transcription (first 30 seconds):\n",
            "----------------------------------------\n",
            "Well, how do you go about making a small rowboat? We just make the small scale model and draft it from that. Make a keel out. You make a scale model first? Most everybody does, make a scale model. Or else they draft them out, draw them out on paper. Either one you want to, it doesn't matter. How big are these scale models? A general rule on small type.\n"
          ]
        }
      ],
      "source": [
        "#Step 3 Transcription without prompts (unguided approach) BASIC\n",
        "import os\n",
        "from pathlib import Path\n",
        "from openai import OpenAI\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# OpenAI client\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Load meeting audio\n",
        "audio_path = Path(\"audio\") / \"ME025.mp3\"\n",
        "audio = AudioSegment.from_file(audio_path)\n",
        "\n",
        "# Take first 30 seconds (30,000 ms)\n",
        "preview = audio[:30_000]\n",
        "\n",
        "# Export preview clip to a temporary WAV file (Whisper-friendly)\n",
        "preview_path = Path(\"audio\") / \"preview_30s.wav\"\n",
        "preview.export(preview_path, format=\"wav\")\n",
        "\n",
        "print(\"ü§ñ Transcribing the first 30 seconds (no chunking, no prompt)...\")\n",
        "\n",
        "with open(preview_path, \"rb\") as f:\n",
        "    transcript = client.audio.transcriptions.create(\n",
        "        model=\"whisper-1\",\n",
        "        file=f\n",
        "    )\n",
        "\n",
        "print(\"\\nüìù Transcription (first 30 seconds):\")\n",
        "print(\"-\" * 40)\n",
        "print(transcript.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "ea0972c3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Step 4: Prompted transcription...\n",
            "‚úÖ Step 4 saved to: transcripts\\step4_ME025_prompted.txt\n"
          ]
        }
      ],
      "source": [
        "#Step 4 Transcription with prompt (guided approach)i\n",
        "import os\n",
        "from pathlib import Path\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "audio_path = Path(\"audio\") / \"ME025.mp3\"\n",
        "if not audio_path.exists():\n",
        "    raise FileNotFoundError(\"‚ùå ME025.mp3 not found\")\n",
        "\n",
        "prompt_text = (\n",
        "    \"This is a discussion about building a small rowboat. \"\n",
        "    \"Topics include making a scale model, drafting plans on paper, \"\n",
        "    \"and boat parts such as the keel. \"\n",
        "    \"Transcribe clearly with proper punctuation and sentence boundaries.\"\n",
        ")\n",
        "\n",
        "print(\"ü§ñ Step 4: Prompted transcription...\")\n",
        "\n",
        "with open(audio_path, \"rb\") as f:\n",
        "    prompted = client.audio.transcriptions.create(\n",
        "        model=\"whisper-1\",\n",
        "        file=f,\n",
        "        prompt=prompt_text\n",
        "    )\n",
        "\n",
        "prompted_text = prompted.text.strip()\n",
        "\n",
        "Path(\"transcripts\").mkdir(exist_ok=True)\n",
        "step4_path = Path(\"transcripts\") / \"step4_ME025_prompted.txt\"\n",
        "step4_path.write_text(prompted_text + \"\\n\", encoding=\"utf-8\")\n",
        "\n",
        "print(\"‚úÖ Step 4 saved to:\", step4_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "3e1a3b1d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Step 5: Unprompted transcription...\n",
            "‚úÖ Step 5 saved to: transcripts\\step5_ME025_unprompted.txt\n"
          ]
        }
      ],
      "source": [
        "#Step 5 Transcription without prompts (unguided approach)\n",
        "import os\n",
        "from pathlib import Path\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "audio_path = Path(\"audio\") / \"ME025.mp3\"\n",
        "if not audio_path.exists():\n",
        "    raise FileNotFoundError(\"‚ùå ME025.mp3 not found\")\n",
        "\n",
        "print(\"ü§ñ Step 5: Unprompted transcription...\")\n",
        "\n",
        "with open(audio_path, \"rb\") as f:\n",
        "    unprompted = client.audio.transcriptions.create(\n",
        "        model=\"whisper-1\",\n",
        "        file=f\n",
        "    )\n",
        "\n",
        "unprompted_text = unprompted.text.strip()\n",
        "\n",
        "step5_path = Path(\"transcripts\") / \"step5_ME025_unprompted.txt\"\n",
        "step5_path.write_text(unprompted_text + \"\\n\", encoding=\"utf-8\")\n",
        "\n",
        "print(\"‚úÖ Step 5 saved to:\", step5_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "920c638f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ STEP 4 ‚Äî PROMPTED\n",
            "------------------------------------------------------------\n",
            "How do you go about making a small rowboat? We just make the small scale model and draft it from that. Make a keel out. You make a scale model first? Most everybody does, make a scale model. Or else they draft them out, draw them out on paper. Either one you want to, it doesn't matter. How big are these scale models? The general rule on small type boat is 3 quarter inch to a foot. The large ones are up to a quarter inch to a foot. And what's the purpose of the scale model? The length and the width and all this. Oh I see, they just use smaller everything and then they just scale them up and down. And then how do you go about deciding to build the boat itself? Well you make the keel first from the model, from the drafting, drawing, whatever it is. Then you make the stem and the stern. And for the small stuff, the small boats where you bend the frame, you make a mold, what we call a mold, there's sections of it, so far apart on the boat you take the shape of it, make sections. What do they use to do that? Plywood? Plywood or cedar, it doesn't matter.\n",
            "\n",
            "\n",
            "üîπ STEP 5 ‚Äî UNPROMPTED\n",
            "------------------------------------------------------------\n",
            "Well, how do you go about making a small rowboat? We just make the small scale model and draft it from that. Make a keel out. You make a scale model first? Most everybody does, make a scale model. Or else they draft them out, draw them out on paper. Either one you want to, it doesn't matter. How big are these scale models? The general rule on small type boat is three-quarter inch to a foot. The large ones are up to a quarter inch to a foot. And what's the purpose of the scale model? Well, to determine the length and the width and all this. Oh, I see. They just use smaller everything and then they just scale them up and down. And then how do you go about deciding to build the boat itself? Well, you make a keel first from the model, from the draftings, drawings, whatever it is. Then you make the stem and the stern. And for the small stuff, the small boats where you bend the frame, you make a mold, what we call a mold, there's sections of it. So far apart on the boat, you take the shape of it, make sections. What do they use to do that? Plywood? Plywood or cedar, it doesn't matter.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Comarison between Prompted (Step 4) and Unprompted (Step 5)\n",
        "from pathlib import Path\n",
        "\n",
        "step4_text = Path(\"transcripts/step4_ME025_prompted.txt\").read_text(encoding=\"utf-8\")\n",
        "step5_text = Path(\"transcripts/step5_ME025_unprompted.txt\").read_text(encoding=\"utf-8\")\n",
        "\n",
        "print(\"üîπ STEP 4 ‚Äî PROMPTED\")\n",
        "print(\"-\" * 60)\n",
        "print(step4_text)\n",
        "\n",
        "print(\"\\nüîπ STEP 5 ‚Äî UNPROMPTED\")\n",
        "print(\"-\" * 60)\n",
        "print(step5_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "d8e17d84",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded audio: Podcast.mp3\n",
            "‚è± Total duration: 28.09 minutes\n",
            "\n",
            "üî™ Created 3 chunk(s) in 'audio\\chunks'\n",
            "\n",
            "üì¶ Chunk verification:\n",
            " - Podcast_chunk_001_0s_to_600s.wav | 600.0 seconds\n",
            " - Podcast_chunk_002_600s_to_1200s.wav | 600.0 seconds\n",
            " - Podcast_chunk_003_1200s_to_1685s.wav | 485.2 seconds\n"
          ]
        }
      ],
      "source": [
        "#Step 6 Implementing chunking for long audio files\n",
        "from pathlib import Path\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Input audio file\n",
        "audio_path = Path(\"audio\") / \"Podcast.mp3\"\n",
        "if not audio_path.exists():\n",
        "    raise FileNotFoundError(\"‚ùå Podcast.mp3 not found in ./audio\")\n",
        "\n",
        "# Output directory for chunks\n",
        "chunks_dir = Path(\"audio\") / \"chunks\"\n",
        "chunks_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Chunk length: 10 minutes (in milliseconds)\n",
        "chunk_length_ms = 10 * 60 * 1000  # 600,000 ms\n",
        "\n",
        "# Load audio\n",
        "audio = AudioSegment.from_file(audio_path)\n",
        "total_duration_ms = len(audio)\n",
        "\n",
        "print(f\"‚úÖ Loaded audio: {audio_path.name}\")\n",
        "print(f\"‚è± Total duration: {total_duration_ms / 1000 / 60:.2f} minutes\")\n",
        "\n",
        "# Split into chunks\n",
        "chunk_paths = []\n",
        "chunk_number = 1\n",
        "\n",
        "for start_ms in range(0, total_duration_ms, chunk_length_ms):\n",
        "    end_ms = min(start_ms + chunk_length_ms, total_duration_ms)\n",
        "    chunk = audio[start_ms:end_ms]\n",
        "\n",
        "    chunk_filename = (\n",
        "        f\"Podcast_chunk_{chunk_number:03d}_\"\n",
        "        f\"{start_ms//1000}s_to_{end_ms//1000}s.wav\"\n",
        "    )\n",
        "    chunk_path = chunks_dir / chunk_filename\n",
        "\n",
        "    chunk.export(chunk_path, format=\"wav\")\n",
        "    chunk_paths.append(chunk_path)\n",
        "\n",
        "    chunk_number += 1\n",
        "\n",
        "print(f\"\\nüî™ Created {len(chunk_paths)} chunk(s) in '{chunks_dir}'\")\n",
        "\n",
        "# Checkpoint verification: list chunks and durations\n",
        "print(\"\\nüì¶ Chunk verification:\")\n",
        "for path in chunk_paths:\n",
        "    c = AudioSegment.from_file(path)\n",
        "    print(f\" - {path.name} | {c.duration_seconds:.1f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ad2273f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded: Podcast.mp3\n",
            "‚è± Total duration: 28.09 minutes\n",
            " - created Podcast_chunk_001_0s_to_600s.wav | 600.0s | 18.31 MB\n",
            " - created Podcast_chunk_002_600s_to_1200s.wav | 600.0s | 18.31 MB\n",
            " - created Podcast_chunk_003_1200s_to_1685s.wav | 485.2s | 14.81 MB\n",
            "\n",
            "‚úÖ Created 3 chunk(s) in audio\\chunks\n"
          ]
        }
      ],
      "source": [
        "#Step 6 with smaller audio chunks to ensure they are under 25MB for Whisper\n",
        "from pathlib import Path\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "audio_path = Path(\"audio\") / \"Podcast.mp3\"\n",
        "chunks_dir = Path(\"audio\") / \"chunks\"\n",
        "chunks_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 10 minutes\n",
        "chunk_length_ms = 10 * 60 * 1000\n",
        "\n",
        "audio = AudioSegment.from_file(audio_path)\n",
        "total_ms = len(audio)\n",
        "\n",
        "print(f\"‚úÖ Loaded: {audio_path.name}\")\n",
        "print(f\"‚è± Total duration: {total_ms/1000/60:.2f} minutes\")\n",
        "\n",
        "chunk_paths = []\n",
        "chunk_number = 1\n",
        "\n",
        "for start_ms in range(0, total_ms, chunk_length_ms):\n",
        "    end_ms = min(start_ms + chunk_length_ms, total_ms)\n",
        "    chunk = audio[start_ms:end_ms]\n",
        "\n",
        "    # üîë Downsample to keep file size < 25MB\n",
        "    chunk = chunk.set_frame_rate(16000).set_channels(1).set_sample_width(2)\n",
        "\n",
        "    chunk_filename = f\"Podcast_chunk_{chunk_number:03d}_{start_ms//1000}s_to_{end_ms//1000}s.wav\"\n",
        "    chunk_path = chunks_dir / chunk_filename\n",
        "    chunk.export(chunk_path, format=\"wav\")\n",
        "\n",
        "    size_mb = os.path.getsize(chunk_path) / (1024 * 1024)\n",
        "    print(f\" - created {chunk_path.name} | {chunk.duration_seconds:.1f}s | {size_mb:.2f} MB\")\n",
        "\n",
        "    chunk_paths.append(chunk_path)\n",
        "    chunk_number += 1\n",
        "\n",
        "print(f\"\\n‚úÖ Created {len(chunk_paths)} chunk(s) in {chunks_dir}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "515dc5cf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Transcribing 3 chunks with timestamps...\n",
            "\n",
            "‚û°Ô∏è Podcast_chunk_001_0s_to_600s.wav (offset 0s)\n",
            "‚û°Ô∏è Podcast_chunk_002_600s_to_1200s.wav (offset 600s)\n",
            "‚û°Ô∏è Podcast_chunk_003_1200s_to_1685s.wav (offset 1200s)\n",
            "\n",
            "‚úÖ Done transcribing all chunks.\n",
            "\n",
            "‚úÖ Saved:\n",
            " - transcripts\\step7_podcast_segments_with_timestamps.json\n",
            " - transcripts\\step7_podcast_timestamped.txt\n",
            " - transcripts\\step7_podcast_full_text.txt\n"
          ]
        }
      ],
      "source": [
        "#Step 7 Transcribing chunks with timestamps\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "chunks_dir = Path(\"audio\") / \"chunks\"\n",
        "chunk_files = sorted(chunks_dir.glob(\"Podcast_chunk_*.wav\"))\n",
        "\n",
        "if not chunk_files:\n",
        "    raise FileNotFoundError(\"‚ùå No chunk files found in audio/chunks\")\n",
        "\n",
        "def parse_chunk_offset_seconds(filename: str) -> int:\n",
        "    \"\"\"\n",
        "    Extract the start offset in seconds from filenames like:\n",
        "    Podcast_chunk_002_600s_to_1200s.wav  -> 600\n",
        "    \"\"\"\n",
        "    m = re.search(r\"_(\\d+)s_to_(\\d+)s\\.wav$\", filename)\n",
        "    if not m:\n",
        "        raise ValueError(f\"Could not parse offset from filename: {filename}\")\n",
        "    return int(m.group(1))\n",
        "\n",
        "all_segments = []\n",
        "combined_text_parts = []\n",
        "\n",
        "print(f\"ü§ñ Transcribing {len(chunk_files)} chunks with timestamps...\\n\")\n",
        "\n",
        "for chunk_path in chunk_files:\n",
        "    offset_s = parse_chunk_offset_seconds(chunk_path.name)\n",
        "    print(f\"‚û°Ô∏è {chunk_path.name} (offset {offset_s}s)\")\n",
        "\n",
        "    with open(chunk_path, \"rb\") as f:\n",
        "        transcript = client.audio.transcriptions.create(\n",
        "            model=\"whisper-1\",\n",
        "            file=f,\n",
        "            response_format=\"verbose_json\",\n",
        "            timestamp_granularities=[\"segment\"]  # segment-level timestamps\n",
        "        )\n",
        "\n",
        "    # Save the chunk's full text (optional but useful)\n",
        "    combined_text_parts.append(transcript.text.strip())\n",
        "\n",
        "    # Adjust segment timestamps by adding the chunk offset\n",
        "    if hasattr(transcript, \"segments\") and transcript.segments:\n",
        "        for seg in transcript.segments:\n",
        "            all_segments.append({\n",
        "                \"start\": float(seg.start) + offset_s,\n",
        "                \"end\": float(seg.end) + offset_s,\n",
        "                \"text\": seg.text.strip(),\n",
        "                \"chunk_file\": chunk_path.name\n",
        "            })\n",
        "\n",
        "print(\"\\n‚úÖ Done transcribing all chunks.\")\n",
        "\n",
        "# Sort segments by time (just to be safe)\n",
        "all_segments.sort(key=lambda x: x[\"start\"])\n",
        "\n",
        "# Create a combined transcript text with timestamps (human-readable)\n",
        "def format_time(seconds: float) -> str:\n",
        "    # hh:mm:ss\n",
        "    seconds = int(round(seconds))\n",
        "    h = seconds // 3600\n",
        "    m = (seconds % 3600) // 60\n",
        "    s = seconds % 60\n",
        "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
        "\n",
        "lines = []\n",
        "for seg in all_segments:\n",
        "    lines.append(f\"[{format_time(seg['start'])} - {format_time(seg['end'])}] {seg['text']}\")\n",
        "\n",
        "combined_timestamped_text = \"\\n\".join(lines)\n",
        "\n",
        "# Save outputs (these are part of Step 7 + help for Step 8)\n",
        "Path(\"transcripts\").mkdir(exist_ok=True)\n",
        "\n",
        "json_path = Path(\"transcripts\") / \"step7_podcast_segments_with_timestamps.json\"\n",
        "txt_path = Path(\"transcripts\") / \"step7_podcast_timestamped.txt\"\n",
        "full_txt_path = Path(\"transcripts\") / \"step7_podcast_full_text.txt\"\n",
        "\n",
        "json_path.write_text(json.dumps(all_segments, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "txt_path.write_text(combined_timestamped_text, encoding=\"utf-8\")\n",
        "full_txt_path.write_text(\"\\n\\n\".join(combined_text_parts), encoding=\"utf-8\")\n",
        "\n",
        "print(f\"\\n‚úÖ Saved:\")\n",
        "print(f\" - {json_path}\")\n",
        "print(f\" - {txt_path}\")\n",
        "print(f\" - {full_txt_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6de73689",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Step 8 exports created:\n",
            " - transcripts\\exports\\podcast_timestamped.txt\n",
            " - transcripts\\exports\\podcast_segments.json\n",
            " - transcripts\\exports\\podcast.srt\n",
            "\n",
            "üîé Preview (first 5 timestamped lines):\n",
            "[00:00:00 - 00:00:02] Hey there, I'm Asma Khalid.\n",
            "[00:00:02 - 00:00:06] And I'm Tristan Redman, and we're here with a bonus episode for you from the Global\n",
            "[00:00:06 - 00:00:07] Story podcast.\n",
            "[00:00:07 - 00:00:09] The world order is shifting.\n",
            "[00:00:09 - 00:00:13] Old alliances are fraying and new ones are emerging.\n",
            "\n",
            "üîé Preview (first SRT cue):\n",
            "1\n",
            "00:00:00,000 --> 00:00:02,000\n",
            "Hey there, I'm Asma Khalid.\n"
          ]
        }
      ],
      "source": [
        "#Step 8 Exporting transcripts in multiple formats (TXT, JSON, SRT)\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Input: Step 7 output\n",
        "segments_path = Path(\"transcripts\") / \"step7_podcast_segments_with_timestamps.json\"\n",
        "if not segments_path.exists():\n",
        "    raise FileNotFoundError(\"‚ùå Step 7 JSON not found. Run Step 7 first.\")\n",
        "\n",
        "segments = json.loads(segments_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "# Output folder\n",
        "export_dir = Path(\"transcripts\") / \"exports\"\n",
        "export_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def format_hhmmss(seconds: float) -> str:\n",
        "    \"\"\"HH:MM:SS for human-readable text exports.\"\"\"\n",
        "    seconds = int(round(seconds))\n",
        "    h = seconds // 3600\n",
        "    m = (seconds % 3600) // 60\n",
        "    s = seconds % 60\n",
        "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
        "\n",
        "def srt_timestamp(seconds: float) -> str:\n",
        "    \"\"\"SRT timestamp format: HH:MM:SS,mmm\"\"\"\n",
        "    if seconds < 0:\n",
        "        seconds = 0\n",
        "    ms = int(round((seconds - int(seconds)) * 1000))\n",
        "    total = int(seconds)\n",
        "    h = total // 3600\n",
        "    m = (total % 3600) // 60\n",
        "    s = total % 60\n",
        "    return f\"{h:02d}:{m:02d}:{s:02d},{ms:03d}\"\n",
        "\n",
        "# ---------- 1) Human-readable TXT with timestamps ----------\n",
        "txt_lines = []\n",
        "for seg in segments:\n",
        "    start = format_hhmmss(seg[\"start\"])\n",
        "    end = format_hhmmss(seg[\"end\"])\n",
        "    text = seg[\"text\"].strip()\n",
        "    txt_lines.append(f\"[{start} - {end}] {text}\")\n",
        "\n",
        "timestamped_txt = \"\\n\".join(txt_lines)\n",
        "txt_path = export_dir / \"podcast_timestamped.txt\"\n",
        "txt_path.write_text(timestamped_txt, encoding=\"utf-8\")\n",
        "\n",
        "# ---------- 2) JSON export (already exists, but we‚Äôll copy a clean version) ----------\n",
        "json_export_path = export_dir / \"podcast_segments.json\"\n",
        "json_export_path.write_text(json.dumps(segments, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "\n",
        "# ---------- 3) SRT export ----------\n",
        "# SRT is: index, time-range line, text, blank line\n",
        "# We'll use each segment as one subtitle cue.\n",
        "srt_blocks = []\n",
        "for i, seg in enumerate(segments, start=1):\n",
        "    start = srt_timestamp(seg[\"start\"])\n",
        "    end = srt_timestamp(seg[\"end\"])\n",
        "    text = seg[\"text\"].strip()\n",
        "\n",
        "    # Optional: avoid empty cues\n",
        "    if not text:\n",
        "        continue\n",
        "\n",
        "    block = f\"{i}\\n{start} --> {end}\\n{text}\\n\"\n",
        "    srt_blocks.append(block)\n",
        "\n",
        "srt_text = \"\\n\".join(srt_blocks)\n",
        "srt_path = export_dir / \"podcast.srt\"\n",
        "srt_path.write_text(srt_text, encoding=\"utf-8\")\n",
        "\n",
        "# ---------- Checkpoint output ----------\n",
        "print(\"‚úÖ Step 8 exports created:\")\n",
        "print(f\" - {txt_path}\")\n",
        "print(f\" - {json_export_path}\")\n",
        "print(f\" - {srt_path}\")\n",
        "\n",
        "print(\"\\nüîé Preview (first 5 timestamped lines):\")\n",
        "for line in timestamped_txt.splitlines()[:5]:\n",
        "    print(line)\n",
        "\n",
        "print(\"\\nüîé Preview (first SRT cue):\")\n",
        "print(srt_text.split(\"\\n\\n\")[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9d7fb10",
      "metadata": {},
      "outputs": [],
      "source": [
        "#to Show what is in Transcrips Folder\n",
        "mport os\n",
        "print(os.listdir(\"transcripts\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9112c49",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ac",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
